%Twitter api, out of requests
\subsection{Twitter API requests}
The initial plan was to use the open Twitter API at the server side to retrieve search results, user info, trending topics etc. The Twitter API consist of several APIs, one for each type of Twitter data each limited to a 150 requests an hour. We discovered quite early in the development process that this number was far too low to be usable in practice. 

A first try at a solution was heavy use of caching of search results (tweets, users and trending topics). The use of caching was also required for user ranking, as we would need to have our own database of users to calculate the ranks. We soon found that the caching solution was not good enough because the ranking algorithm requires the storage of information about which users follow a user, and which users a user follows. Each search thus required up to 41 requests to the user API, almost a third of the available requests, which amounts to about three searches an hour.

Finally, we decided to move all user and search requests client side. As a result, the amount of information the server can cache is only limited to the amount of users that use the system. It should be noted that the problem has only been moved client side and as a result, users are now only able to do about four searches an hour. As our database grows however, the restriction should be somewhat alleviated.

%Server probl
\subsection{Server Problems}

%fake data probl
\subsection{JSON data injection}
Due to the Twitter API request limit and the resulting AJAX client workaround we have in essence opened our application to fraudulent input from the client. Since we have no control over the client, and due to the fact that the users have full control, they will be able to construct fake, but valid JSON data to send to our controllers. Basically this means that they will be able to create fake users, user data (such as descriptions), and fake the tables lising followers and following. They will also be able to dictate the content to show in the tweets we display.

In order to make use of this security glitch a user would simply have to look at the data sent from the FeedJam frontend to the backend, and make a similar POST using the same JSON format. In order to find the requests one would simply have to open the developer tools in Chrome/Safari and click the network tab, which keeps track of all network usage. Here all requests sent to our server will be listed including the appropriate HTTP headers includeing the URL which the request is sent to and our JSON in plain text. After this it would be a simple matter to write a script which sends POST requests to the correct URL (which our controller is listening to) with correctly formatted data.

There is no way that we can ensure that such fraudulent data does not make its way into our database outside of conducting all the queries server side, which as previously explained proved to be a bad solution due to the request limit. While it is possible to insert correctly formatted data, it is not possible to inject SQL statements due to our use of named parameters. In addition to this the fraudulent data problem is somewhat alleviated by our revisiting policy which ensures that this fraudulent data will be exchanged for new data after a period of 4 weeks.
